{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis and Visualization with Python and the NLTK\n",
    "\n",
    "This notebook was originally prepared for use during a workshop called \"An Introduction to Visualizing Text with Python,\" which took place during [Columbia's Art of Data Visualization week](http://library.columbia.edu/news/events/data-visualization.html) in April 2016. But you can run these commands yourself. To begin, you'll need this software: \n",
    "\n",
    " * Python 3\n",
    " * These Python 3 packages (make sure you don't install their Python 2 versions): \n",
    "   - Jupyter (formerly called iPython Notebook) \n",
    "   - NLTK (the Natural Language Processing Toolkit) \n",
    "   - Pandas (a data science library) \n",
    "   - Wordcloud\n",
    "\n",
    "There are lots of different ways to get this software. You can either install it on your computer, or run it in the cloud. Here are a few different ways of doing that. When you see text in a `monospace typeface`, those are commands to be entered in the terminal. On a Mac, open a terminal by typing \"Terminal\" into Spotlight. On Windows, press Win+R and type `cmd` to get a terminal.  \n",
    "\n",
    "## Installation on Linux\n",
    "\n",
    "1. Make sure your package list is up to date: `sudo apt-get update`\n",
    "1. Get Python 3: `sudo apt-get install python3 python3-pip python3-pandas`\n",
    "2. Get the Python packages: `sudo pip3 install jupyter nltk wordcloud`\n",
    "3. Start a Jupyter notebook: `jupyter notebook`\n",
    "\n",
    "## Installation on Mac or Windows: \n",
    "\n",
    "1. Get Anaconda with Python 3 from https://www.continuum.io/downloads\n",
    "2. Anaconda comes with Pandas, NLTK, and Jupyter, so just install Wordcloud: `conda install --name wordcloud`\n",
    "3. Start a Jupyter notebook: `jupyter notebook`\n",
    "\n",
    "## Or Use DHBox\n",
    "\n",
    "1. Go to http://dhbox.org and click \"log in.\" Log into the workshop box with the credentials I gave earlier. \n",
    "2. In the \"dataviz-2017\" profile menu in the top right, click \"Apps.\" \n",
    "2. Click the \"Jupyter Notebook\" tab. \n",
    "\n",
    "<!-- \n",
    "## Installation on DHBox \n",
    "\n",
    "DHBox is a platform for running digital humanities (DH) software in the cloud. \n",
    "\n",
    "1. Make a DHBox account on http://dhbox.org, by clicking \"sign up.\"  2. In your user menu in the upper-right corner, select \"Apps.\" \n",
    "3. Click the tab \"Command Line.\" \n",
    "4. Enter the commands from the section \"Installation on Linux\" above. \n",
    "5. Click the tab \"Jupyter Notebooks,\" and enter your password again. \n",
    "\n",
    "One you have all the software installed, you can run the commands below either by copying and pasting them from this notebook, or by running them directly in this notebook, by downloading the notebook and opening it with jupyter (i.e. `jupyter notebook dataviz-workshop.ipynb`). \n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the Natural Language Processing Toolkit\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.download('book') # You only need to run this command once, to get the NLTK book data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the data science package Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Get the library matplotlib for making pretty charts\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Make plots appear here in this notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# This just makes the plot size bigger, so that we can see it easier. \n",
    "plt.rcParams['figure.figsize'] = (12,4)\n",
    "\n",
    "# We'll use the OS module to download things. \n",
    "import os\n",
    "\n",
    "# Get all the example books from the NLTK textbook\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work with our Own Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download Alice in Wonderland\n",
    "os.system('wget http://www.gutenberg.org/files/11/11-0.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize it (break it into words), and make an NLTK Text object out of it. \n",
    "aliceRaw = open('11-0.txt').read()\n",
    "aliceWords = nltk.word_tokenize(aliceRaw)\n",
    "alice = nltk.Text(aliceWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Texts\n",
    "\n",
    "Let's explore these texts a little. There are lots of things we can do with these texts. \n",
    "To see a list, type `text1.` and press `<Tab>`. One thing we can do is look at statistically significant co-occurring two-word phrases, here known as *collocations*: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we get tired of doing that for each text, and want to do it with all of them? \n",
    "Let's put the texts into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alltexts = [text1, text2, text3, text4, text5, text6, text7, text8, text9, alice]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at it to make sure it's all there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alltexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of all the texts, we can loop through each one, running the `collocations()` function on each: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for text in alltexts:     # For each text in the list \"alltexts,\"\n",
    "    text.collocations()   # Get the collocations\n",
    "    print('---')          # Print a divider between the collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances and Dispersion Plots\n",
    "\n",
    "Now let's look up an individual word in a text, and have NLTK give us some context: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.concordance('shrubbery')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. But what if we want to see visually where those words occur over the course of the text? We can use the function `dispersion_plot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text6.dispersion_plot(['shrubbery', 'ni'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try that on Moby Dick: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text1.dispersion_plot(['Ahab', 'Ishmael', 'whale'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at dispersion plots of characters' names, we can almost tell which characters in Sense and Sensibility have romantic relationships: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2.dispersion_plot(['Elinor', 'Marianne', 'Edward', 'Willoughby'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Text Vocabulary\n",
    "\n",
    "We can use the `len` (length) function to count the total number of words in a text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can do this for all the texts by putting it in a lookup function, like this: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengths = {text.name: len(text) for text in alltexts}\n",
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we import this table into Pandas, we can see this data a little easier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by plotting it, we can get a better visual representation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(lengths).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But word counts themselves are not very interesting, so let's see if we can not only count the words, \n",
    "but count the vocabulary of a text. To do that, we can use `set()`, which will count every word once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "porky_sentence = \"the the the the the that's all folks\"\n",
    "porky_words = porky_sentence.split()\n",
    "porky_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the words in the sentence easily: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(porky_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count the words, but ignore repeated words, we can use the function set(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set(porky_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we count this set, we can determine the vocabulary of a text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(porky_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can find the vocabulary of Moby Dick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(set(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty big, but then again, Moby Dick is kind of a long novel. \n",
    "We can adjust for the words by adjusting for the total words: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(text1) / len(set(text1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would get tedious if we did this for every text, so let's write a function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vocab(text):                       # Define a function called `vocab` that takes the input `text` \n",
    "    return len(text) / len(set(text))  # Divide the number of words by the number of unique words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vocab(porky_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go through each text, and get its vocabulary, and put it in a table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabularies = {text.name: vocab(text) for text in alltexts}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that table into Pandas so we can see it better: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(vocabularies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.Series(vocabularies).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, now let's make a famous wordcloud from a text. This just takes the most statistically significant words, and plots them where the size of each word corresponds to its frequency. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud # Get the library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawtext = ' '.join(text1.tokens) # Stitch it back together. \n",
    "wc = WordCloud(width=1000, height=600, background_color='white').generate(rawtext)\n",
    "plt.axis('off')                          # Turn off axis ticks\n",
    "plt.imshow(wc, interpolation=\"bilinear\");# Plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Words (Conditional Frequency Distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at the inaugural address corpus in detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import inaugural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set up a conditional word frequency distribution for it, \n",
    "pairing off a list of words with the list of inaugural addresses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (14,5) # Adjust the plot size. \n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (target, fileid[:4])\n",
    "           for fileid in inaugural.fileids()\n",
    "           for w in inaugural.words(fileid)\n",
    "           for target in ['america', 'citizen']\n",
    "           if w.lower().startswith(target))\n",
    "cfd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can replace the words 'america' and 'citizen' here with whatever words you want, to further explore this corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's play around with the Brown corpus. It's a categorized text corpus. Let's see all the categories: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nltk.corpus.brown.categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create another conditional frequency distribution, this time based on these genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "genres = ['adventure', 'romance', 'science_fiction']\n",
    "words = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "cfdist = nltk.ConditionalFreqDist(\n",
    "              (genre, word)\n",
    "              for genre in genres\n",
    "              for word in nltk.corpus.brown.words(categories=genre)\n",
    "              if word in words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cfdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can plot these words by genre: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cfdist).T.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Play around with this CFD a bit by changing the genres and words used above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further resources\n",
    "\n",
    "To learn more, check out the NLTK book (from which a lot of the examples here were adapted): http://nltk.org/book\n",
    "\n",
    "To see what's possible with more advanced techniques, using the [SpaCy](http://spacy.io) library, check out [my workshop notebook in advanced text analysis](https://github.com/JonathanReeve/advanced-text-analysis-workshop-2017/blob/master/advanced-text-analysis.ipynb). \n",
    "\n",
    "Read about some experiments in text analysis on my blog: http://jonreeve.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
